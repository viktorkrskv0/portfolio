{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viktor Karsakov 205727183\n",
    "# Guy Leitersdorfersdorf 201178977"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Download the \"Boston1.csv\" database, and explore the data. Explanation about the dataset can be found here: http://www.clemson.edu/economics/faculty/wilson/R-tutorial/analyzing_data.html\n",
    "\n",
    "Find the columns with missing values and filter them out of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 15)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lin = LinearRegression()\n",
    "\n",
    "# boston = load_boston()\n",
    "# df = pd.DataFrame(data = boston.data, columns = boston.feature_names)\n",
    "# df['medv'] = pd.Series(boston.target)\n",
    "# df[\"medv\"].unique()\n",
    "\n",
    "df = pd.read_csv(\"boston1.csv\")\n",
    "df.dropna(axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Divide the filtered data randomly into a train set (70% of the data) and test set (30% of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3)\n",
    "#test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't done this previously, install the scikit-learn package for python.\n",
    "\n",
    "a) On the train set, run a linear regression model as follows:\n",
    "Divide the training set into explanatory variables (the X matrix with which we'll try to make a prediction) and a target variable (y, the value which we'll try to predict). Use the 'medv' attribute as the target variable y and the rest of the features as the X matrix. Run a linear regression model on those sets, and print the regression coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR REGRESSION COEFFICIENTS:  [[-1.10754867e-01  4.34751566e-02  4.51336323e-02  3.34670125e+00\n",
      "  -1.88203513e+01  3.41102081e+00 -1.35332426e-03 -1.46194395e+00\n",
      "   2.79557321e-01 -1.06912270e-02 -9.77065289e-01  8.70483535e-03\n",
      "  -5.68348959e-01  5.66375771e-01]]\n"
     ]
    }
   ],
   "source": [
    "#     print(train.columns.values)\n",
    "train_cols = list(train.columns.values)\n",
    "train_cols.remove(\"medv\")\n",
    "\n",
    "X = train.loc[:,train_cols]\n",
    "Y = train.loc[:,[\"medv\"]]\n",
    "Y.shape\n",
    "\n",
    "test_X = test.loc[:, train_cols]\n",
    "test_Y = test.loc[:, [\"medv\"]]\n",
    "    \n",
    "res = lin.fit(X,Y)\n",
    "print(\"LINEAR REGRESSION COEFFICIENTS: \",res.coef_ )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Use the linear regression model to predict the values of the test set's 'medv' column, based on the test set's other attributes. Print the Mean Squared Error of the model on the train set and on the test set.\n",
    "Usually, the MSE on the train set would be lower than the MSE on the test set, since the model parameters are optimized with respect to the train set. Must this always be the case? Can you think of a few examples for when this might not be the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Train =  24.027665757126535\n",
      "MSE Test =  17.299262009336775\n"
     ]
    }
   ],
   "source": [
    "pred_y = lin.predict(X)\n",
    "pred_test_y = lin.predict(test_X)\n",
    "\n",
    "MSE_train = mean_squared_error(Y, pred_y)\n",
    "MSE_test = mean_squared_error(test_Y, pred_test_y)\n",
    "\n",
    "print(\"MSE Train = \", MSE_train)\n",
    "print(\"MSE Test = \", MSE_test)\n",
    "\n",
    "#     MSE of the test can be lower than the train in some cases, where the samples of the test are close to the linear \n",
    "#     regression surface. \n",
    "#     For example, given a surface of a training set, let us look at one point on it, mark it as X. \n",
    "#     Now lets assume that the entire test set are samples of the same point X\n",
    "#     We know that X is predeicted with 100% accuracy (since its on the surface), namely 0 MSE.\n",
    "#     We also know that not all of the train samples of train set are X, thus the MSE isn't 0.\n",
    "#     Hence, Test MSE < Train MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Add some noise (with mean=0, std=1) to the test set's y, and predict it again. What happened to the MSE? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR REGRESSION WITH NOISE:  18.496822335868977\n"
     ]
    }
   ],
   "source": [
    "noise = np.random.normal(0,1,[152,1])\n",
    "noisy_test_Y = test_Y + noise\n",
    "\n",
    "print(\"LINEAR REGRESSION WITH NOISE: \", mean_squared_error(noisy_test_Y, pred_test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Create a Recursive feature elimination model, with a linear regression estimator, that selects half of the original number of features. Hint: Check the feature_selection module in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(estimator=lin) #,  n_features_to_select=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Use the feature elimination model on the full database (after filtering columns with missing values, before partitioning into train/test). Print the features that were selected. Remember that we separate the 'medv' attribute to be our y, while the rest of the attributes in the dataset serve as features to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Note that the features that were used are the ones set on 'True' \n",
      "crim  | used:  False | rank:  3\n",
      "zn  | used:  False | rank:  5\n",
      "indus  | used:  False | rank:  4\n",
      "chas  | used:  True | rank:  1\n",
      "nox  | used:  True | rank:  1\n",
      "rm  | used:  True | rank:  1\n",
      "age  | used:  False | rank:  8\n",
      "dis  | used:  True | rank:  1\n",
      "rad  | used:  False | rank:  2\n",
      "tax  | used:  False | rank:  6\n",
      "ptratio  | used:  True | rank:  1\n",
      "black  | used:  False | rank:  7\n",
      "lstat  | used:  True | rank:  1\n",
      "randCol  | used:  True | rank:  1\n"
     ]
    }
   ],
   "source": [
    "cols = list(df.columns.values)\n",
    "# print(cols)\n",
    "cols.remove(\"medv\")\n",
    "X_full = df.loc[:,train_cols]\n",
    "Y_full = df.loc[:,[\"medv\"]]\n",
    "y_full = np.ravel(Y_full)\n",
    "rfe.fit(X_full, y_full)\n",
    "j=0\n",
    "print(\"Please Note that the features that were used are the ones set on 'True' \")\n",
    "for i in cols:\n",
    "    print(i, \" | used: \", rfe.support_[j], \"| rank: \", rfe.ranking_[j])\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) We'd like to find out the optimal number of features. Create feature elimination models (with linear regression estimators) for every number of features between 1 and n (where n = all the original features, 'medv' excluded). For each number of features, run a linear regression as in Question 2, only on the selected features, in order to predict 'medv'. Print the Mean Sqaured Error for each number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 1 ==============\n",
      "MSE - 64.617369\n",
      "number of features used - 1 \n",
      "============== 2 ==============\n",
      "MSE - 34.580085\n",
      "number of features used - 2 \n",
      "============== 3 ==============\n",
      "MSE - 32.350879\n",
      "number of features used - 3 \n",
      "============== 4 ==============\n",
      "MSE - 28.111022\n",
      "number of features used - 4 \n",
      "============== 5 ==============\n",
      "MSE - 26.066170\n",
      "number of features used - 5 \n",
      "============== 6 ==============\n",
      "MSE - 19.104018\n",
      "number of features used - 6 \n",
      "============== 7 ==============\n",
      "MSE - 19.103194\n",
      "number of features used - 7 \n",
      "============== 8 ==============\n",
      "MSE - 18.920702\n",
      "number of features used - 8 \n",
      "============== 9 ==============\n",
      "MSE - 18.870616\n",
      "number of features used - 9 \n",
      "============== 10 ==============\n",
      "MSE - 18.605714\n",
      "number of features used - 10 \n",
      "============== 11 ==============\n",
      "MSE - 18.112242\n",
      "number of features used - 11 \n",
      "============== 12 ==============\n",
      "MSE - 17.415735\n",
      "number of features used - 12 \n",
      "============== 13 ==============\n",
      "MSE - 16.611654\n",
      "number of features used - 13 \n"
     ]
    }
   ],
   "source": [
    "y = np.ravel(Y)\n",
    "for i in range(1,14):\n",
    "    print(\"==============\", i, \"==============\" )\n",
    "    rfe = RFE(estimator=lin,  n_features_to_select=i)\n",
    "    rfe.fit(X_full,np.ravel(Y_full))\n",
    "    pred_y = rfe.predict(test_X)\n",
    "    print(\"MSE - %f\" % mean_squared_error(test_Y, np.ravel(pred_y)))\n",
    "    print(\"number of features used - %d \" % rfe.n_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Conclude the optimal number of features for this task. Think about the cost of adding for data vs the benefit of a more accurate prediction. Explain your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In conclusion, from examining the above results, we see that the lowest MSE is alternating between 9 features and 12 features due to randomness. we don't get any additional, significant accuracy beyond that. running learning models on a lot of features is expansive, and If we don't gain any more knowledge, it is usually better to choose the minimal number of features. please also note that it also helps us to avoid the curse of dimensionality. there for we choose 9 features, which gives us very low MSE\n"
     ]
    }
   ],
   "source": [
    "print(\"In conclusion, from examining the above results, we see that the lowest MSE is alternating between 9 features and 12 features due to randomness. we don't get any additional, significant accuracy beyond that. running learning models on a lot of features is expansive, and If we don't gain any more knowledge, it is usually better to choose the minimal number of features. please also note that it also helps us to avoid the curse of dimensionality. there for we choose 9 features, which gives us very low MSE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a cross-validation of the linear regression on the train set with K=5. Print the CV scores for each repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.64016305  0.70491511  0.58645003  0.07720145 -0.27230699]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(lin, X_full,np.ravel(Y_full), cv=5)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
